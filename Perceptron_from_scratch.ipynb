{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1530373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cae7f35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508bbb72",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c80618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain,ytrain),(xtest,ytest) = keras.datasets.mnist.load_data()\n",
    "class preper():\n",
    "    def __init__(self):\n",
    "        return None\n",
    "        \n",
    "    def flatten(self,data):\n",
    "        c=[]\n",
    "        for i in data:\n",
    "            c.append((i.reshape(1,784)/256)-0.5)\n",
    "        return c\n",
    "    def onehot(self,data):\n",
    "        c=[]\n",
    "        for i in data:\n",
    "            a=np.zeros((1,10))\n",
    "            a=a[0]\n",
    "            a[i]=1\n",
    "            c.append(a)\n",
    "        return c\n",
    "\n",
    "preper=preper()\n",
    "xtrain=preper.flatten(xtrain)\n",
    "ytrain=preper.onehot(ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f2fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a30774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e711417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self,arc=[784,16,16,10]):\n",
    "        self.W=[]\n",
    "        self.arc=arc\n",
    "        [self.W.append(np.random.random((self.arc[x],self.arc[x+1]))-0.5)for x in range(len(self.arc))[:-1]]\n",
    "        self.B=[np.random.random((1,i))-0.5 for i in self.arc[1:]]\n",
    "        print(self.B)#deletable\n",
    "        self.probstack=[]#deletable\n",
    "        self.Wstacktester=[] #deletable\n",
    "        self.testerflag=True #deletable\n",
    "        self.testerww=[] #deletable\n",
    "        #print(self.W)\n",
    "        #print(self.B)\n",
    "        \n",
    "    def FF(self,inp):\n",
    "        self.O=[inp]#list of arrays\n",
    "        self.Z=[inp]#list of arrays\n",
    "        \n",
    "        def relu(x):\n",
    "            return np.maximum(0,x)\n",
    "        def sigmoid(x):\n",
    "            x=np.exp(x)\n",
    "            s=np.sum(x)\n",
    "            return x/s\n",
    "        def da(w,b):\n",
    "            #print(self.O[-1])\n",
    "            self.Z.append(self.O[-1]@w+b)#append an array\n",
    "            self.O.append(relu(self.Z[-1]))\n",
    "            \n",
    "        [da(w,b) for w,b in zip(self.W,self.B)]\n",
    "        self.O[-1]= sigmoid(self.Z[-1])\n",
    "        #print(self.B)#deletable\n",
    "        return self.O[-1]\n",
    "        \n",
    "    def batchify(self,data):\n",
    "        return [data[x:x+100] for x in range(len(data))[::100]]\n",
    "    \n",
    "    def renorm(self,a,b):\n",
    "        c=[]\n",
    "        for i,thing in enumerate(a):\n",
    "            if not(isinstance(i,type(np.array([1]))) or isinstance(i,list)):\n",
    "                c.append(thing+b[i])\n",
    "            else:\n",
    "                c.append(self.renorm(thing,b[i]))\n",
    "        #self.c=c# deletable\n",
    "        return c\n",
    "    \n",
    "    def renorm2(self,j,length):\n",
    "        k=[]\n",
    "        \n",
    "        for i in j:\n",
    "            if not(isinstance(i,type(np.array([1])))or isinstance(i,list)):\n",
    "                k.append(i/length)\n",
    "            else:\n",
    "                k.append(self.renorm2(i,length))\n",
    "        return k\n",
    "    \n",
    "    def destack(self,stack):\n",
    "        j=stack[0]\n",
    "        for i in stack[1:]:\n",
    "            j=self.renorm(j,i)\n",
    "        #return [np.array(i) for i in self.renorm2(j,len(stack)) ]\n",
    "        return self.renorm2(j,len(stack))\n",
    "    \n",
    "    def BP(self,batch):\n",
    "        deltaBstack=[]\n",
    "        deltaWstack=[]\n",
    "        if self.testerflag :\n",
    "            self.testerflag=False\n",
    "            self.testerww= self.B # why the fuck is this []?\n",
    "        ###hoping the batch is list with items of form [(image,label)]..\n",
    "        for I,i in batch:\n",
    "            self.FF(I)\n",
    "            dCdO=(self.O[-1]-i)*2\n",
    "            dOdZ=np.diagflat(self.O[-1]*(1-self.O[-1]))\n",
    "            dCdZ=dCdO@dOdZ\n",
    "            #self.probstack.append((problem,I,i,self.O[-1]))#deletable\n",
    "            def drelu(x):\n",
    "                if x>0:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "            drelu=np.vectorize(drelu)\n",
    "            biastemplate=self.B.copy()\n",
    "            weighttemplate=self.W.copy()\n",
    "\n",
    "            for layer in range(len(self.arc))[::-1][:-1]:# 3,2,1 \n",
    "                frm,to=weighttemplate[layer-1].shape\n",
    "                weighttemplate[layer-1]=np.tile(dCdZ,(frm,1))*np.tile(self.O[layer-1].T,(1,to))\n",
    "                biastemplate[layer-1]=dCdZ@np.eye(dCdZ.shape[1])\n",
    "                if not(layer==1):\n",
    "                    dCdO=dCdZ@self.W[layer-1].T\n",
    "                    dCdZ=dCdO*drelu(self.Z[layer-1])\n",
    "                \n",
    "            deltaBstack.append(biastemplate) \n",
    "            deltaWstack.append(weighttemplate) \n",
    "            \n",
    "        B=self.destack(deltaBstack)\n",
    "        self.Wstacktester=deltaWstack\n",
    "        W=self.destack(deltaWstack)\n",
    "        alpha=-0.1\n",
    "        B=[np.array(b)*alpha for b in B]\n",
    "        #self.testerw.append(self.W)\n",
    "        W=[np.array(w)*alpha for w in W]\n",
    "        #self.testerww.append(self.W)\n",
    "        self.B=self.renorm(self.B,B)\n",
    "        self.W=self.renorm(self.W,W)\n",
    "        \n",
    "    def accuracy(self,batch):\n",
    "        out=[(self.FF(I)-i)**2 for I,i in batch]\n",
    "        return np.sum(out)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "062d4daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-0.28138943, -0.48137174, -0.17860834,  0.47774382, -0.01266057,\n",
      "         0.20910158, -0.42225397, -0.35678191,  0.09564004,  0.09139158,\n",
      "         0.05126811, -0.12340529,  0.46810755, -0.39608702,  0.21750917,\n",
      "         0.05542765]]), array([[-0.02948465,  0.15048114,  0.14960271, -0.025214  , -0.01570899,\n",
      "        -0.10267969,  0.35368965,  0.10892471,  0.29331177, -0.17864567,\n",
      "         0.15283427,  0.33982424,  0.03755565,  0.45646471,  0.05887733,\n",
      "         0.07751781]]), array([[ 0.01346062, -0.4013631 ,  0.13135487, -0.32890035, -0.09969454,\n",
      "         0.05764184, -0.05526691,  0.21565025,  0.03455624,  0.1862326 ]])]\n"
     ]
    }
   ],
   "source": [
    "nn=NN()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc80847b",
   "metadata": {},
   "source": [
    "                if(layer!=len(self.arc)-1):\n",
    "                    biastemplate[layer-1]=drelu(self.Z[layer])*problem\n",
    "                    step=np.tile(self.O[-1]*(1-self.O[-1]),(self.O[-1].shape[1],1))*np.eye(self.O[-1].shape[1])\n",
    "                    step2=problem@step \n",
    "                else:\n",
    "                    #problem@(dOl/dZl)@(dZl/dBl)\n",
    "                    #self.eye=self.O[-1]\n",
    "                    step=np.tile(self.O[-1]*(1-self.O[-1]),(self.O[-1].shape[1],1))*np.eye(self.O[-1].shape[1])\n",
    "                    step2=problem@step \n",
    "                    biastemplate[layer-1]=step2@np.eye(self.O[-1].shape[1])\n",
    "                    self.layers=[self.O[layer].shape for layer in range(4)]#deltable\n",
    "                    \n",
    "                self.testerlayer=self.O[layer].shape\n",
    "                weighttemplate[layer-1]=np.tile(step2,(self.O[layer-1].shape[1],1))*np.tile(self.O[layer-1].T,(1,self.O[layer].shape[1]))\n",
    "\n",
    "                problem=drelu(self.Z[layer]*problem)@self.W[layer-1].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39745649",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=list(zip(xtrain,ytrain))\n",
    "batched=nn.batchify(data) #[ [(image,label),(),(),()], [(image,label),(),(),()], .............] image is a 1*748 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a00138a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105.16879644561678"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.accuracy(batched[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd99dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.69734333e-05, 2.75723861e-05, 2.57692448e-03, 1.52651019e-06,\n",
       "        3.52987962e-03, 2.67247458e-02, 9.66713381e-01, 2.16677560e-04,\n",
       "        4.17616767e-08, 1.92277069e-04]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.FF(batched[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aee3b1d7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[nn.BP(batched[i]) for i in range(len(batched))]\n",
    "print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9821e3b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.981595889788714"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.accuracy(batched[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a1c7d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.34062888, -0.47547913,  0.25699628, -0.13121289, -0.07783348,\n",
       "          0.34131684, -0.14596772,  0.2224481 ,  0.3159467 , -0.08238976,\n",
       "          0.27122278,  0.24014582, -0.40431421,  0.15036854,  0.45898189,\n",
       "         -0.01967173]]),\n",
       " array([[-0.05126031,  0.17501898,  0.49401787, -0.27982131, -0.21293722,\n",
       "          0.42663296,  0.08762134,  0.14802933,  0.06462902,  0.15573227,\n",
       "          0.46846541, -0.45385251,  0.04688685, -0.01622721,  0.09620134,\n",
       "          0.3255319 ]]),\n",
       " array([[-0.08916168, -0.31935458, -0.14253289, -0.0944817 , -0.37158285,\n",
       "          0.10909869,  0.17375277,  0.24089925,  0.39472371, -0.18780483]])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.testerww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e45994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched[0][0][0] is the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4cb8f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b31819e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 5), (0, 0), (1, 4), (1, 1), (9, 9), (4, 2), (1, 1), (3, 3), (1, 1), (4, 4), (3, 3), (1, 5), (3, 3), (6, 6), (1, 1), (9, 7), (2, 2), (8, 8), (6, 6), (7, 9), (4, 4), (0, 0), (4, 9), (1, 1), (6, 1), (3, 2), (9, 4), (3, 3), (7, 2), (9, 7), (8, 3), (3, 8), (6, 6), (9, 9), (0, 0), (5, 5), (6, 6), (0, 0), (9, 7), (6, 6), (1, 1), (5, 8), (7, 7), (9, 9), (1, 3), (9, 9), (9, 8), (3, 5), (5, 9), (3, 3), (3, 3), (0, 0), (7, 7), (8, 4), (7, 9), (8, 8), (0, 0), (9, 9), (4, 4), (1, 1), (4, 4), (4, 4), (6, 6), (0, 0), (4, 4), (3, 5), (6, 6), (1, 1), (3, 0), (0, 0), (1, 1), (7, 7), (1, 1), (6, 6), (3, 3), (3, 0), (3, 2), (1, 1), (4, 1), (7, 7), (0, 9), (0, 0), (2, 2), (6, 6), (7, 7), (3, 8), (7, 3), (9, 9), (0, 0), (9, 4), (6, 6), (7, 7), (4, 4), (6, 6), (8, 8), (0, 0), (7, 7), (8, 8), (3, 3), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "def indexofmax(i):\n",
    "    m=max(i)\n",
    "    for j,thing in enumerate(i):\n",
    "        if m==thing:\n",
    "            return j\n",
    "def func0(I,i):\n",
    "    x=nn.FF(I)[0]\n",
    "    xx=indexofmax(x)\n",
    "    y=indexofmax(i)\n",
    "    return (xx,y)\n",
    "    \n",
    "print([func0(I,i) for I,i in batched[0]]) #predictions and real labels seem to match most of the times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4cd644e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45898189"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad3d2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1)\n",
      "(1, 2)\n",
      "(2, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print((i,thing)) for i,thing in enumerate([1,2,3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af994b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4509b844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59049"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x:x**5)(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869f394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceaa71f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
